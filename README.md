# Qhapac-Ñan #

#### Research problem  
There is vast research on the long term effects of social or economic institutions (Dell, 2010 or Albertus et al. 2020 for examples focused in Perú). One significant institution whose long term effects have not been much researched is the ‘Incan trail’ (Camino Inca or Qhaphac Ñan in Quechua). The Qhapac Ñan (QÑ onwards) was a large roads system built by the Incas in the XV century, with an extension of over 30,000 km and covering Perú, Colombia, Ecuador, Bolivia, Chile and Argentina. In the ancient time, where transportation was much harder, proximity to this road system could make a huge difference in economic development. Moreover, at least in Perú, over time some of these old roads were replaced by highways, reinforcing the importance of proximity to them.  
  
While there is one attempt to unravel the long term effects of the QÑ (Franco et al, 2021), this paper methodology has shortcomings. First, the selection of the geographical area they focus is arbitrary. Second, the study uses the ‘ring method’ to define their treatment and control areas. Using different sized buffers, they define treatment areas as those who are X km or closer to the QÑ route and control areas as those who are in the next X km. However, this ignores that because of how complex is Peruvian geography, there are areas where there is only one possible road to go from one point to another. This means that areas contiguous to the QÑ routes are not necessarily comparable to the areas where the QÑ goes through. In turn, this would mean that their estimates are biased (because of selection bias) and that their results cannot be considered causal effects.  
  
An alternative approach to estimate these effects could be to simulate alternative QÑ routes and use them to create control group areas. This can be done using a Least Cost Path (LCP) analysis and geographic variables such as height, slope, proximity to rivers or valleys. Nevertheless, the LCP needs weights for each of these variables to simulate routes. Assigning these weights is key to the analysis, as it determines the resulting route. For instance, assigning a big weight to height would make the LCP to create a route that avoids the highlands of Perú as much as it can. This way, it becomes a crucial issue to understand the relative importance of these variables in determining the real QÑ. Moreover, their importance can also vary depending on each route.  
  
To solve this issue, we prepared a dataset following this steps. First, we selected specific QÑ routes for the analysis. Based on historical texts, we identified the most important population and religion centers in the Incan era and selected the QÑ routes that connected them. This resulted in 16 QÑ routes. Second, instead of just using the QÑ routes that connected these locations, we generated straight lines that connected them and drew 10km grid buffers around them. Then, we divided each grid using 50 by 50 meters cells to allow for very granular analysis. This resulted in a 11.6 million observations (rows) dataset. Third, for each of these small cells, we calculated the following variables:  
  
    - qapac (binary): equals 1 if the cell is 300m or less from the QÑ
    - height (continuous): average height of the cell (in meters)
    - slope (continuous): average slope of the cell (in degrees)
    - rivers (binary): equals 1 if the cell is at 100m or less from a river
    - valley (binary): equals 1 if the cell is at 100m or less from a valley
    - ID (categorical): route ID (16 unique values)
    - prox (continuous): distance to the closest QÑ point  
  
The height and slope variables have straightforward explanations. Height in the Peruvian highlands can go up to almost 7km over the sea level. Such height can have negative health effects, especially if you are travelling by foot, which was common for the messengers in the Incan empire. Slope is also very relevant as steep slope can make traveling by foot much more difficult. Proximity to rivers and valleys is also important because it meant access to food: fish in the case of rivers and crops for valleys. Proximity to valleys is not trivial because valleys are almost the only areas with large plots of rich agricultural land in Perú. Agriculture was (and is) also practiced in other areas but mostly in small plots and complex geographical conditions.  
  
Using ‘qapac’ as the dependent variable and height, slope, rivers and valleys as independent variables, a simple linear regression model can be used to get an idea of the relative importance of these variables in determining the QÑ. However, there are two sources of variation that could affect this estimates. First, estimated coefficients may vary considerably by route, as these are located in different geographical zones. Second, proximity to the real QÑ is also relevant. Thus, this linear regression model will be executed (i) for the whole dataset, filtering data by proximity to the actual QÑ using 1km steps (from 1km to 10km), and (ii) for each route, also filtering the data by proximity to the actual QÑ. Before presenting the results of these analyses, the following sections discuss the scalable computing methods employed and their importance.  
  
#### Importance of using scalable computing methods  
The nature of the analysis, which implies repeated linear regressions for every combination of route ID and buffer distance, implies fitting more than 100 models. Moreover, the dataset itself contains approximately 11.6 million observations, and this whole dataset is used for one model. Performing this analysis on a local computer versus in a cloud or remote cluster can make a significant difference in the following items:  
  
1.	Memory constraints: Handling multiple gigabytes of geospatial data, especially with feature engineering and filtering, can exceed the available memory.  
2.	Computation time: Each regression must be computed on filtered subsets of data. The sequential execution of all combinations would most likely lead to long execution times.  
3.	Parallelism Potential: The independence of each model fitting (route and buffer combinations) makes this ideal for parallel computing.  
4.	Scalability: Further analysis, such as adding variables would most likely make the analysis unmanageable locally.  
5.	Cloud storage: If data is stored and processed in the cloud, this avoids costly and slow data transfers.  
  
Thus, leveraging parallel and cloud tools can greatly impact the efficiency of this analysis.  
  
#### Large-scale computing methods employed
The two large-scale computing tools employed are AWS (EC2 and S3) and a Dask cluster (also cloud-based). Below is a more detailed description of how are these employed.  

Cloud-based distributed cluster: an EC2 Dask cluster was deployed using the Dask AWS cloud provider. The cluster includes a scheduler and eight worker nodes (r5.large instances) with multiple vCPUs and large RAM allocations. This allows for efficient parallel computing across the cluster. Also, as the data is stored in an AWS S3 bucket, all data operations and model fitting are carried out within the cluster. Still, intermediate results are saved locally as they are not memory consuming (just plots saved as png files).
  
Dask Dataframes and Arrays: the dataset is loaded and processed using Dask dataframes, which internally partition the data across the number of workers. Modeling tasks are conducted using Dask arrays and Dask-specific ML libraries, which are designed to work with Dask automated parallelism. In addition, memory optimization is increased using .persist(), which reduces redundant computation and accelerates repeated access to the same subsets. Un-doing the persist after each iteration of the loops also prevents inefficient usage memory usage.  
  
#### Brief discussion of results  
Some insights can be taken from the plots ([pooled](https://github.com/macs30123-s24/final-project-qhapac_nan/blob/main/coeffs_pooled.png) and [by route](https://github.com/macs30123-s24/final-project-qhapac_nan/blob/main/coeffs_routes.png)) in [analysis.ipynb](https://github.com/macs30123-s24/final-project-qhapac_nan/blob/main/analysis.ipynb):  
1.	In the pooled routes analysis, the ordering of the variables in terms of importance is slope, rivers, valleys and height. This is regardless of the buffer filtering. However, the importance of slope decreases from 90% to 40% as the buffer size increases. The importance of rivers varies within 15% and 40%, and the importance of valleys and height increase consistently from 0 to almost 20% as the buffer size increases.  
2.	The route specific analysis confirms that the main variables can differ by route. This suggests using different weights by route for the LCP analysis could be crucial. At the same time, the weird pattern that some variables exhibit in many routes (such as rivers in route 2 or height and rivers in route 7) suggest that results by route are not that robust. Still, there are also routes where all variables show a consistent pattern as buffer size increases, such as routes 1 and 8.  
  
**Disclaimers**  
The initial project also included fitting ML models after the linear regression analysis. However, several bugs prevented this. First, I kept having issues to load the data from S3. Second, standardizing the variables and then joining them to the data frame would repeatedly generate index errors, which is why I ended up using concatenation. Third, in the models by route, I kept getting errors. It turned out these errors were because after filtering the data by route and buffer size, some partitions would get empty. After trying many different things, I figured the best way to handle these cases was to gather data in one partition and then re-distribute again, but this took me a while.  
  
Finally, the data I used was prepared by someone else that I used to work with as part of a research project. This colleague was a geographer and, while I trust them, I did not review their process when we were working together. Thus, if time allowed, I wanted to re-create the data as part of this project. As I wanted to be sure to have something to submit, I started using the data I already had, and unfortunately did not have time to do this. Still, I am looking forward to try it in the future.